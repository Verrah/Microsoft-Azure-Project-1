# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data related to marketing campaigns of a banking institutions. The data contains bank client attributes, campaign contact attributes and the socio-economic attributes. We seek to predict if a client to the bank will subscribe to the term deposit.

The best performing model was from the “VotingEnsemble” algorithm with an accuracy of 0.91733.
  
## Scikit-learn Pipeline
The pipeline architecture used in the prediction process was in the following major steps: creating a workspace & experiment, creating the compute cluster, parameter sampling, and early termination policy Hyperdrive configuration.

* The workspace "quick-starts-ws-128450" was defined to provide a centralized space for managing several artifacts necessary for the ML workloads. This included the history of all training runs, logs, metrics output and a snapshot of the scripts used. The information stored in the workspace are critical for determining which training run generates the best model.

* The experiment "first-udacity-project" was also created to represent the collection of runs used to validate the prediction.
The “udacity-project-one” compute cluster which was necessary for running the training script and management of the model was created. The cluster vm_size was “Standard_D2_V2 with maximum number of nodes running concurrently set to 4.

* The “RandomParameterSampling” algorithm was used to define the search space with two parameters; the first being “—C” which was a uniform distribution with 0 as the minimum value and 1 as the maximum value. The second parameter “—max_iter” was a choice of [50, 100, 150, 200].

* To improve the computational efficiency, the bandit policy was considered for early termination. The policy was applied at every interval when the metrics are reported. Any run whose best metric was less than 91% of the best performing run would then be terminated.

* The SKLearn estimator was used to learn from the train data which is a classification algorithm – specifically logistic regression algorithm. Finally, the Hyperdrive configuration was created using the estimator, hyperparameter sampler and the early termination policy among other specifications. The Hyperdrive configurations allowed for an automated hyperparameter tuning process during training. The specified code configured the experiment to use a maximum of 40 runs, concurrently four runs at a time.
  
## AutoML Model 
The experiment parameter and model training settings were defined and linked to the AutoML config as a **kwargs** parameter. For a shorter run time, the “experiment_timeout_minutes” was set to 30 minutes. Additionally, the model task was set to “classification”.

The results of the AutoML generated the best model algorithm – “Voting Ensemble” – with an accuracy of 0.91773. The voting ensemble combined predictions from multiple other models to achieve better performance than any single model used in the ensemble. The best individual pipeline score was 0.91632 with the best individual iteration being 32.

The main hyperparameters generated by the best run in the AutoML process were

* ensembled iterations: [32, 0, 1, 28, 36, 14, 34, 15, 4]
* ensembled algorithms: ['XGBoostClassifier', 'LightGBM', 'XGBoostClassifier', 'XGBoostClassifier', 'XGBoostClassifier', 'RandomForest', 'XGBoostClassifier', 'SGD', 'SGD']
* ensemble weights': '[0.07142857142857142, 0.07142857142857142, 0.2857142857142857, 0.07142857142857142, 0.14285714285714285, 0.14285714285714285, 0.07142857142857142, 0.07142857142857142, 0.07142857142857142]

## Pipeline comparison
The Hyperdrive config and AutoML have near similar accuracy: 0.91394 and 0.91773 respectively. After 40 runs the hyperparameter values for the Hyperdrive best performance was 0.87738 regularization strength for 150 iterations. AutoML run for 39 runs until timeout with slightly better results compared to the Hyperdrive.

Both Hyperdrive and AutoML achieve near similar results thus suitable for this marketing campaign problem and dataset. In terms of architecture, the Hyperdrive required specifying the algorithm that works best with the different specified hyperparameters. The Hyperdrive automated the testing of the specified hyperparameters. The AutoML, on the other hand, achieved competitive results with minimal effort thus not requiring specifications for the algorithm to be tested.

## Future work
Both the Hyperdrive and AutoML tools are useful. It would be great to also compare their performance on other complex problems like Deep Neural Networks, Neural Architecture Search among others
